{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR-iU2pUHdPm"
      },
      "source": [
        "# Étape 2 : Évaluation du modèle de base (AVANT fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwfkN1NjKuZV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\" Device utilisé: {device}\")\n",
        "\n",
        "# Mapping des labels pour Allociné (binaire)\n",
        "label_mapping = {0: 'Négatif', 1: 'Positif'}\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp4c4MISK5Cu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CHARGEMENT DES DONNÉES ALLOCINÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CHARGEMENT DES DONNÉES ALLOCINÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Charger les données de l'étape précédente\n",
        "try:\n",
        "    train_df = pd.read_csv('train_data_allocine.csv')\n",
        "    test_df = pd.read_csv('test_data_allocine.csv')\n",
        "    print(f\" Données Allociné chargées avec succès !\")\n",
        "    print(f\"   Train: {len(train_df)} avis\")\n",
        "    print(f\"   Test: {len(test_df)} avis\")\n",
        "except FileNotFoundError:\n",
        "    print(\" Erreur: Fichiers de données Allociné non trouvés.\")\n",
        "    raise\n",
        "\n",
        "# Aperçu des données de test qu'on va évaluer\n",
        "print(f\"\\n Aperçu des données de test :\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Statistiques sur les données\n",
        "print(f\"\\n Statistiques des données :\")\n",
        "print(f\"   Longueur moyenne train: {train_df['text'].str.len().mean():.0f} caractères\")\n",
        "print(f\"   Longueur moyenne test: {test_df['text'].str.len().mean():.0f} caractères\")\n",
        "print(f\"   Mots moyens train: {train_df['text'].str.split().str.len().mean():.1f}\")\n",
        "print(f\"   Mots moyens test: {test_df['text'].str.split().str.len().mean():.1f}\")\n",
        "\n",
        "# Distribution des classes\n",
        "print(f\"\\n Distribution des classes :\")\n",
        "train_dist = train_df['label'].value_counts().sort_index()\n",
        "test_dist = test_df['label'].value_counts().sort_index()\n",
        "\n",
        "for label_id in [0, 1]:\n",
        "    train_count = train_dist.get(label_id, 0)\n",
        "    test_count = test_dist.get(label_id, 0)\n",
        "    train_pct = train_count / len(train_df) * 100\n",
        "    test_pct = test_count / len(test_df) * 100\n",
        "    print(f\"   {label_mapping[label_id]}: Train {train_count} ({train_pct:.1f}%) | Test {test_count} ({test_pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U-D4VsdLPNV"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CHARGEMENT DU MODÈLE PRÉ-ENTRAÎNÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CHARGEMENT DU MODÈLE DE BASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# On va tester plusieurs modèles pour comparaison\n",
        "models_to_test = [\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "    \"camembert-base\"  # Modèle français spécialisé\n",
        "]\n",
        "\n",
        "# Commencer par DistilBERT multilingue\n",
        "model_name = \"distilbert-base-multilingual-cased\"\n",
        "print(f\" Chargement de {model_name}...\")\n",
        "\n",
        "try:\n",
        "    # Charger le tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Charger le modèle avec la bonne configuration pour classification binaire\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,  # 2 classes : négatif, positif\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    print(f\" Modèle chargé avec succès sur {device}\")\n",
        "    print(f\" Nombre de paramètres: {model.num_parameters():,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Erreur lors du chargement: {e}\")\n",
        "    # Fallback vers un modèle plus simple\n",
        "    print(\" Essai avec un modèle alternatif...\")\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    model.to(device)\n",
        "    print(f\" Modèle de fallback chargé: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJKhffx0LcKu"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CRÉATION D'UN PIPELINE DE CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CRÉATION DU PIPELINE DE CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Créer un pipeline pour faciliter les prédictions\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "print(\" Pipeline créé avec succès !\")\n",
        "\n",
        "# Test rapide du pipeline avec des avis réalistes\n",
        "test_examples = [\n",
        "    \"Ce film est absolument magnifique ! Les acteurs sont formidables.\",\n",
        "    \"Film décevant, scénario prévisible et jeu d'acteur médiocre.\",\n",
        "    \"Un chef-d'œuvre du cinéma français, émotionnellement bouleversant.\"\n",
        "]\n",
        "\n",
        "print(f\"\\n Tests rapides du pipeline :\")\n",
        "for i, text in enumerate(test_examples, 1):\n",
        "    try:\n",
        "        result = classifier(text)\n",
        "        best_pred = max(result, key=lambda x: x['score'])\n",
        "        print(f\"{i}. \\\"{text[:50]}...\\\"\")\n",
        "        print(f\"   → {best_pred['label']} (confiance: {best_pred['score']:.3f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"{i}. Erreur sur l'exemple {i}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YvskH9wLr8d"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FONCTION D'ÉVALUATION OPTIMISÉE\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model_batch(texts, true_labels, classifier, batch_size=16, verbose=True):\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur un ensemble de données par batch pour plus d'efficacité\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\" Évaluation en cours par batch...\")\n",
        "\n",
        "    predictions = []\n",
        "    all_scores = []\n",
        "\n",
        "    # Traitement par batch\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        if verbose and (i // batch_size + 1) % 5 == 0:\n",
        "            print(f\"   Batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
        "\n",
        "        try:\n",
        "            # Prédiction du batch\n",
        "            batch_results = classifier(batch_texts)\n",
        "\n",
        "            for result in batch_results:\n",
        "                # Trouver la classe avec la plus haute probabilité\n",
        "                best_pred = max(result, key=lambda x: x['score'])\n",
        "                pred_label = label2id[best_pred['label']]\n",
        "                predictions.append(pred_label)\n",
        "\n",
        "                # Stocker tous les scores\n",
        "                scores = {item['label']: item['score'] for item in result}\n",
        "                all_scores.append(scores)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur sur le batch {i//batch_size + 1}: {e}\")\n",
        "            # Ajouter des prédictions par défaut\n",
        "            for _ in batch_texts:\n",
        "                predictions.append(0)  # Default négatif\n",
        "                all_scores.append({'NEGATIVE': 0.5, 'POSITIVE': 0.5})\n",
        "\n",
        "    # Calculer les métriques\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predictions, average='weighted'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'all_scores': all_scores,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOWobZxKL3R3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ÉVALUATION SUR LES DONNÉES DE TEST ALLOCINÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ÉVALUATION DU MODÈLE DE BASE SUR ALLOCINÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Préparer les données pour l'évaluation\n",
        "test_texts = test_df['text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "print(f\" Évaluation sur {len(test_texts)} avis de films...\")\n",
        "\n",
        "# Évaluer le modèle avec traitement par batch\n",
        "baseline_results = evaluate_model_batch(test_texts, test_labels, classifier, batch_size=8)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"\\n RÉSULTATS DU MODÈLE DE BASE SUR ALLOCINÉ :\")\n",
        "print(f\"   Précision (Accuracy): {baseline_results['accuracy']:.3f} ({baseline_results['accuracy']*100:.1f}%)\")\n",
        "print(f\"   Précision (Precision): {baseline_results['precision']:.3f}\")\n",
        "print(f\"   Rappel (Recall): {baseline_results['recall']:.3f}\")\n",
        "print(f\"   F1-Score: {baseline_results['f1']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ToL56XMeFA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ANALYSE DÉTAILLÉE DES RÉSULTATS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ANALYSE DÉTAILLÉE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(test_labels, baseline_results['predictions'])\n",
        "print(f\"\\n Matrice de confusion :\")\n",
        "print(\"    Prédictions →\")\n",
        "print(\"Réel ↓   Nég  Pos\")\n",
        "for i, row in enumerate(cm):\n",
        "    label_name = label_mapping[i][:3]\n",
        "    print(f\"{label_name}     {row[0]:3d}  {row[1]:3d}\")\n",
        "\n",
        "# Visualisation de la matrice de confusion\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Matrice de confusion\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            xticklabels=['Négatif', 'Positif'],\n",
        "            yticklabels=['Négatif', 'Positif'],\n",
        "            cmap='Blues')\n",
        "plt.title('Matrice de confusion - Modèle de base\\n(Dataset Allociné)')\n",
        "plt.xlabel('Prédictions')\n",
        "plt.ylabel('Vraies étiquettes')\n",
        "\n",
        "# Distribution des prédictions\n",
        "plt.subplot(2, 2, 2)\n",
        "pred_counts = pd.Series(baseline_results['predictions']).value_counts().sort_index()\n",
        "labels = [label_mapping[i] for i in pred_counts.index]\n",
        "plt.bar(labels, pred_counts.values, color=['red', 'green'])\n",
        "plt.title('Distribution des prédictions')\n",
        "plt.ylabel('Nombre de prédictions')\n",
        "\n",
        "# Scores de confiance\n",
        "plt.subplot(2, 2, 3)\n",
        "confidences = []\n",
        "for scores in baseline_results['all_scores']:\n",
        "    max_conf = max(scores.values())\n",
        "    confidences.append(max_conf)\n",
        "\n",
        "plt.hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(np.mean(confidences), color='red', linestyle='--',\n",
        "           label=f'Moyenne: {np.mean(confidences):.3f}')\n",
        "plt.xlabel('Score de confiance')\n",
        "plt.ylabel('Fréquence')\n",
        "plt.title('Distribution des scores de confiance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Performance par classe\n",
        "plt.subplot(2, 2, 4)\n",
        "class_metrics = []\n",
        "for label_id in [0, 1]:\n",
        "    mask = np.array(test_labels) == label_id\n",
        "    if mask.sum() > 0:\n",
        "        class_predictions = np.array(baseline_results['predictions'])[mask]\n",
        "        class_accuracy = (class_predictions == label_id).mean()\n",
        "        class_metrics.append(class_accuracy)\n",
        "    else:\n",
        "        class_metrics.append(0)\n",
        "\n",
        "plt.bar([label_mapping[i] for i in [0, 1]], class_metrics, color=['red', 'green'])\n",
        "plt.title('Accuracy par classe')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Rapport de classification détaillé\n",
        "print(f\"\\n Rapport de classification détaillé :\")\n",
        "class_names = ['Négatif', 'Positif']\n",
        "report = classification_report(\n",
        "    test_labels,\n",
        "    baseline_results['predictions'],\n",
        "    target_names=class_names,\n",
        "    digits=3\n",
        ")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKdynsiHMpp9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ANALYSE DES ERREURS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ANALYSE DES ERREURS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Créer un DataFrame avec les résultats\n",
        "results_df = test_df.copy()\n",
        "results_df['predicted'] = baseline_results['predictions']\n",
        "results_df['correct'] = results_df['label'] == results_df['predicted']\n",
        "\n",
        "# Ajouter les scores de confiance\n",
        "confidences = [max(scores.values()) for scores in baseline_results['all_scores']]\n",
        "results_df['confidence'] = confidences\n",
        "\n",
        "# Avis mal classifiés\n",
        "wrong_predictions = results_df[~results_df['correct']]\n",
        "correct_predictions = results_df[results_df['correct']]\n",
        "\n",
        "print(f\" Erreurs de classification :\")\n",
        "print(f\"   Prédictions correctes: {len(correct_predictions)}/{len(results_df)} ({len(correct_predictions)/len(results_df)*100:.1f}%)\")\n",
        "print(f\"   Prédictions incorrectes: {len(wrong_predictions)}/{len(results_df)} ({len(wrong_predictions)/len(results_df)*100:.1f}%)\")\n",
        "\n",
        "# Exemples d'erreurs par catégorie\n",
        "print(f\"\\n Exemples d'erreurs :\")\n",
        "for true_label in [0, 1]:\n",
        "    errors_for_label = wrong_predictions[wrong_predictions['label'] == true_label]\n",
        "    if len(errors_for_label) > 0:\n",
        "        print(f\"\\n{label_mapping[true_label]} mal classifiés comme {label_mapping[1-true_label]} :\")\n",
        "        for idx, row in errors_for_label.head(2).iterrows():\n",
        "            text_preview = row['text'][:100] + \"...\" if len(row['text']) > 100 else row['text']\n",
        "            print(f\"  • \\\"{text_preview}\\\"\")\n",
        "            print(f\"    → Confiance: {row['confidence']:.3f}\")\n",
        "\n",
        "# Exemples de bonnes prédictions\n",
        "print(f\"\\n Exemples de prédictions correctes :\")\n",
        "for true_label in [0, 1]:\n",
        "    correct_for_label = correct_predictions[correct_predictions['label'] == true_label]\n",
        "    if len(correct_for_label) > 0:\n",
        "        # Prendre les plus confiantes\n",
        "        best_correct = correct_for_label.nlargest(2, 'confidence')\n",
        "        print(f\"\\n{label_mapping[true_label]} bien classifiés :\")\n",
        "        for idx, row in best_correct.iterrows():\n",
        "            text_preview = row['text'][:100] + \"...\" if len(row['text']) > 100 else row['text']\n",
        "            print(f\"  • \\\"{text_preview}\\\"\")\n",
        "            print(f\"    → Confiance: {row['confidence']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X4KHcw_M9tm"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ANALYSE DE LA CONFIANCE DU MODÈLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ANALYSE DE LA CONFIANCE DU MODÈLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "avg_confidence = np.mean(confidences)\n",
        "avg_confidence_correct = correct_predictions['confidence'].mean() if len(correct_predictions) > 0 else 0\n",
        "avg_confidence_wrong = wrong_predictions['confidence'].mean() if len(wrong_predictions) > 0 else 0\n",
        "\n",
        "print(f\" Confiance moyenne globale: {avg_confidence:.3f}\")\n",
        "print(f\" Confiance sur prédictions correctes: {avg_confidence_correct:.3f}\")\n",
        "print(f\" Confiance sur prédictions incorrectes: {avg_confidence_wrong:.3f}\")\n",
        "\n",
        "confidence_gap = avg_confidence_correct - avg_confidence_wrong\n",
        "print(f\" Écart de confiance (correct vs incorrect): {confidence_gap:.3f}\")\n",
        "\n",
        "if confidence_gap > 0.1:\n",
        "    print(\" Le modèle est plus confiant sur ses bonnes prédictions\")\n",
        "else:\n",
        "    print(\" Le modèle a des difficultés à évaluer sa propre confiance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImohLAelNFPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAUVEGARDE DES RÉSULTATS DE BASE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAUVEGARDE DES RÉSULTATS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sauvegarder les résultats détaillés\n",
        "results_df.to_csv('baseline_results_allocine.csv', index=False)\n",
        "\n",
        "# Sauvegarder les métriques\n",
        "baseline_metrics = {\n",
        "    'model_name': model_name,\n",
        "    'dataset': 'allocine',\n",
        "    'test_size': len(test_df),\n",
        "    'accuracy': baseline_results['accuracy'],\n",
        "    'precision': baseline_results['precision'],\n",
        "    'recall': baseline_results['recall'],\n",
        "    'f1': baseline_results['f1'],\n",
        "    'avg_confidence': avg_confidence,\n",
        "    'avg_confidence_correct': avg_confidence_correct,\n",
        "    'avg_confidence_wrong': avg_confidence_wrong\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('baseline_metrics_allocine.json', 'w') as f:\n",
        "    json.dump(baseline_metrics, f, indent=2)\n",
        "\n",
        "print(f\" Résultats sauvegardés :\")\n",
        "print(f\"   • baseline_results_allocine.csv : prédictions détaillées\")\n",
        "print(f\"   • baseline_metrics_allocine.json : métriques de performance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B_SJMVWKa3v"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RÉCAPITULATIF\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" RÉCAPITULATIF DE L'ÉVALUATION BASELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\" Modèle évalué: {model_name}\")\n",
        "print(f\" Dataset: Allociné ({len(test_df)} avis de test)\")\n",
        "print(f\" Performance baseline :\")\n",
        "print(f\"   • Accuracy: {baseline_results['accuracy']*100:.1f}%\")\n",
        "print(f\"   • F1-Score: {baseline_results['f1']:.3f}\")\n",
        "print(f\"   • Confiance moyenne: {avg_confidence:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}