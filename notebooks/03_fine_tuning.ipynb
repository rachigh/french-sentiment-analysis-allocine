{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR-iU2pUHdPm"
      },
      "source": [
        "# Étape 3 : Fine-tuning du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaY7zo66ODoa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\" Device utilisé: {device}\")\n",
        "\n",
        "# Mapping des labels pour Allociné\n",
        "label_mapping = {0: 'Négatif', 1: 'Positif'}\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjdqPdrCOKm7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CHARGEMENT DES DONNÉES ALLOCINÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CHARGEMENT DES DONNÉES ALLOCINÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Charger les données\n",
        "train_df = pd.read_csv('train_data_allocine.csv')\n",
        "test_df = pd.read_csv('test_data_allocine.csv')\n",
        "\n",
        "print(f\" Données chargées :\")\n",
        "print(f\"   Train: {len(train_df)} avis de films\")\n",
        "print(f\"   Test: {len(test_df)} avis de films\")\n",
        "\n",
        "# Vérifier la distribution\n",
        "print(f\"\\n Distribution des classes dans l'entraînement :\")\n",
        "train_counts = train_df['label'].value_counts().sort_index()\n",
        "for label, count in train_counts.items():\n",
        "    percentage = count / len(train_df) * 100\n",
        "    print(f\"   {label_mapping[label]}: {count} avis ({percentage:.1f}%)\")\n",
        "\n",
        "# Statistiques sur la longueur des textes\n",
        "train_df['text_length'] = train_df['text'].str.len()\n",
        "test_df['text_length'] = test_df['text'].str.len()\n",
        "\n",
        "print(f\"\\n Statistiques des textes :\")\n",
        "print(f\"   Longueur moyenne train: {train_df['text_length'].mean():.0f} caractères\")\n",
        "print(f\"   Longueur médiane train: {train_df['text_length'].median():.0f} caractères\")\n",
        "print(f\"   Longueur max train: {train_df['text_length'].max()} caractères\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FYR9EL6OYpw"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PRÉPARATION DU MODÈLE ET TOKENIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PRÉPARATION DU MODÈLE POUR LE FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_name = \"distilbert-base-multilingual-cased\"\n",
        "print(f\" Chargement de {model_name} pour fine-tuning...\")\n",
        "\n",
        "# Charger le tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Charger le modèle FRAIS\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\" Modèle et tokenizer chargés\")\n",
        "print(f\" Nombre de paramètres: {model.num_parameters():,}\")\n",
        "\n",
        "# Analyser la longueur des tokens pour optimiser max_length\n",
        "print(f\"\\n Analyse de la tokenisation...\")\n",
        "sample_texts = train_df['text'].sample(n=100).tolist()\n",
        "token_lengths = []\n",
        "\n",
        "for text in sample_texts:\n",
        "    tokens = tokenizer(text, truncation=False)['input_ids']\n",
        "    token_lengths.append(len(tokens))\n",
        "\n",
        "max_length = int(np.percentile(token_lengths, 95))  # 95% des textes\n",
        "max_length = min(max_length, 512)  # Limiter à 512 pour la mémoire\n",
        "\n",
        "print(f\" Longueur optimale de tokenisation: {max_length} tokens\")\n",
        "print(f\"   (couvre 95% des textes sans troncature excessive)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50KKMFA4OiVT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PRÉPARATION DES DATASETS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PRÉPARATION DES DATASETS POUR L'ENTRAÎNEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenise les textes pour l'entraînement\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding=False,  # Padding dynamique\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "# Créer les datasets Hugging Face\n",
        "print(\" Conversion en format Dataset...\")\n",
        "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
        "\n",
        "# Tokeniser les datasets\n",
        "print(\" Tokenisation...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Configurer les colonnes pour l'entraînement\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Définir le format pour PyTorch\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print(f\" Datasets préparés :\")\n",
        "print(f\"   Train: {len(train_dataset)} exemples\")\n",
        "print(f\"   Test: {len(test_dataset)} exemples\")\n",
        "\n",
        "# Aperçu d'un exemple tokenisé\n",
        "print(f\"\\n Exemple tokenisé :\")\n",
        "example = train_dataset[0]\n",
        "original_text = train_df.iloc[0]['text']\n",
        "print(f\"   Text original: \\\"{original_text[:100]}...\\\"\")\n",
        "print(f\"   Input IDs shape: {example['input_ids'].shape}\")\n",
        "print(f\"   Label: {example['labels']} ({label_mapping[example['labels'].item()]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "916EP-qzOn_b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION DE L'ENTRAÎNEMENT OPTIMISÉE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CONFIGURATION DE L'ENTRAÎNEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Créer le répertoire de sortie\n",
        "output_dir = \"./fine_tuned_model_allocine\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Configuration optimisée pour le dataset Allociné\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=200,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "print(f\" Configuration d'entraînement optimisée :\")\n",
        "print(f\"   Époques: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size effectif: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
        "print(f\"   Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"   FP16 activé: {training_args.fp16}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80hOqaq3OwNq"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MÉTRIQUES D'ÉVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calcule les métriques d'évaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculer les métriques\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted'\n",
        "    )\n",
        "\n",
        "    # Métriques par classe\n",
        "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None, labels=[0, 1]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_negative': f1_per_class[0],\n",
        "        'f1_positive': f1_per_class[1],\n",
        "        'precision_negative': precision_per_class[0],\n",
        "        'precision_positive': precision_per_class[1]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxxa6oM9O11D"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CRÉATION DU TRAINER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CRÉATION DU TRAINER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Data collator pour le padding dynamique\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Créer le trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\" Trainer créé avec succès !\")\n",
        "print(f\" Dataset d'entraînement: {len(train_dataset)} exemples\")\n",
        "print(f\" Dataset d'évaluation: {len(test_dataset)} exemples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZLricCDO8-D"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LANCEMENT DE L'ENTRAÎNEMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" LANCEMENT DU FINE-TUNING SUR ALLOCINÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\" Début de l'entraînement...\")\n",
        "print(\" Entraînement sur des données réelles d'avis de films...\")\n",
        "print(\" Objectif: Améliorer significativement vs baseline\")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n Entraînement terminé !\")\n",
        "print(f\" Résultats finaux :\")\n",
        "print(f\"   Loss finale: {train_result.training_loss:.4f}\")\n",
        "print(f\"   Temps d'entraînement: {train_result.metrics['train_runtime']:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1QvVjIGPCDb"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ÉVALUATION DU MODÈLE FINE-TUNÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ÉVALUATION DU MODÈLE FINE-TUNÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Évaluer sur le test set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\" Métriques finales sur le test set :\")\n",
        "print(f\"   Accuracy: {eval_results['eval_accuracy']:.3f} ({eval_results['eval_accuracy']*100:.1f}%)\")\n",
        "print(f\"   F1-Score global: {eval_results['eval_f1']:.3f}\")\n",
        "print(f\"   Precision: {eval_results['eval_precision']:.3f}\")\n",
        "print(f\"   Recall: {eval_results['eval_recall']:.3f}\")\n",
        "print(f\"   Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "print(f\"\\n Métriques par classe :\")\n",
        "print(f\"   F1-Score Négatif: {eval_results['eval_f1_negative']:.3f}\")\n",
        "print(f\"   F1-Score Positif: {eval_results['eval_f1_positive']:.3f}\")\n",
        "print(f\"   Precision Négatif: {eval_results['eval_precision_negative']:.3f}\")\n",
        "print(f\"   Precision Positif: {eval_results['eval_precision_positive']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF50cg4pPQyI"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPARAISON AVEC LE MODÈLE DE BASE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" COMPARAISON AVANT/APRÈS FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Charger les métriques de base\n",
        "try:\n",
        "    with open('baseline_metrics_allocine.json', 'r') as f:\n",
        "        baseline_metrics = json.load(f)\n",
        "\n",
        "    print(f\" AMÉLIORATION OBTENUE :\")\n",
        "    print(f\"   Accuracy: {baseline_metrics['accuracy']:.3f} → {eval_results['eval_accuracy']:.3f}\")\n",
        "    print(f\"   F1-Score: {baseline_metrics['f1']:.3f} → {eval_results['eval_f1']:.3f}\")\n",
        "    print(f\"   Precision: {baseline_metrics['precision']:.3f} → {eval_results['eval_precision']:.3f}\")\n",
        "    print(f\"   Recall: {baseline_metrics['recall']:.3f} → {eval_results['eval_recall']:.3f}\")\n",
        "\n",
        "    # Calculer les améliorations\n",
        "    acc_improvement = (eval_results['eval_accuracy'] - baseline_metrics['accuracy']) * 100\n",
        "    f1_improvement = (eval_results['eval_f1'] - baseline_metrics['f1']) * 100\n",
        "\n",
        "    print(f\"\\n GAINS ABSOLUS :\")\n",
        "    print(f\"   Accuracy: {acc_improvement:+.1f} points de pourcentage\")\n",
        "    print(f\"   F1-Score: {f1_improvement:+.1f} points\")\n",
        "\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\" Métriques de base non trouvées\")\n",
        "    baseline_metrics = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXv4E2AiPYG7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALISATION DES AMÉLIORATIONS\n",
        "# ============================================================================\n",
        "\n",
        "if baseline_metrics:\n",
        "    print(\"\\n Création du graphique de comparaison...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Comparaison des métriques principales\n",
        "    ax1 = axes[0, 0]\n",
        "    metrics_names = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
        "    baseline_values = [\n",
        "        baseline_metrics['accuracy'],\n",
        "        baseline_metrics['f1'],\n",
        "        baseline_metrics['precision'],\n",
        "        baseline_metrics['recall']\n",
        "    ]\n",
        "    finetuned_values = [\n",
        "        eval_results['eval_accuracy'],\n",
        "        eval_results['eval_f1'],\n",
        "        eval_results['eval_precision'],\n",
        "        eval_results['eval_recall']\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax1.bar(x - width/2, baseline_values, width, label='Baseline', color='lightcoral')\n",
        "    bars2 = ax1.bar(x + width/2, finetuned_values, width, label='Fine-tuné', color='lightgreen')\n",
        "\n",
        "    ax1.set_xlabel('Métriques')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Comparaison des performances\\nBaseline vs Fine-tuning')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(metrics_names)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    def autolabel(ax, rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.3f}',\n",
        "                       xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                       xytext=(0, 3),\n",
        "                       textcoords=\"offset points\",\n",
        "                       ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    autolabel(ax1, bars1)\n",
        "    autolabel(ax1, bars2)\n",
        "\n",
        "    # 2. Évolution de l'accuracy\n",
        "    ax2 = axes[0, 1]\n",
        "    models = ['Baseline\\n(DistilBERT)', 'Fine-tuned\\n(DistilBERT)', 'État de l\\'art\\n(CamemBERT)']\n",
        "    accuracies = [baseline_metrics['accuracy']*100, eval_results['eval_accuracy']*100, 97.44]\n",
        "    colors = ['red', 'orange', 'green']\n",
        "\n",
        "    bars = ax2.bar(models, accuracies, color=colors, alpha=0.7)\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Progression vers l\\'état de l\\'art')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 3. F1-Score par classe\n",
        "    ax3 = axes[1, 0]\n",
        "    classes = ['Négatif', 'Positif']\n",
        "    f1_baseline = [baseline_metrics['f1'], baseline_metrics['f1']]\n",
        "    f1_finetuned = [eval_results['eval_f1_negative'], eval_results['eval_f1_positive']]\n",
        "\n",
        "    x_classes = np.arange(len(classes))\n",
        "    bars1 = ax3.bar(x_classes - width/2, f1_baseline, width, label='Baseline', color='lightcoral')\n",
        "    bars2 = ax3.bar(x_classes + width/2, f1_finetuned, width, label='Fine-tuné', color='lightgreen')\n",
        "\n",
        "    ax3.set_xlabel('Classes')\n",
        "    ax3.set_ylabel('F1-Score')\n",
        "    ax3.set_title('F1-Score par classe')\n",
        "    ax3.set_xticks(x_classes)\n",
        "    ax3.set_xticklabels(classes)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Amélioration absolue\n",
        "    ax4 = axes[1, 1]\n",
        "    improvements = [\n",
        "        acc_improvement,\n",
        "        f1_improvement,\n",
        "        (eval_results['eval_precision'] - baseline_metrics['precision']) * 100,\n",
        "        (eval_results['eval_recall'] - baseline_metrics['recall']) * 100\n",
        "    ]\n",
        "\n",
        "    colors_improvement = ['green' if x > 0 else 'red' for x in improvements]\n",
        "    bars = ax4.bar(metrics_names, improvements, color=colors_improvement, alpha=0.7)\n",
        "    ax4.set_ylabel('Amélioration (points)')\n",
        "    ax4.set_title('Gains absolus du fine-tuning')\n",
        "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    for bar, improvement in zip(bars, improvements):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2,\n",
        "                bar.get_height() + (0.5 if improvement > 0 else -1),\n",
        "                f'{improvement:+.1f}', ha='center',\n",
        "                va='bottom' if improvement > 0 else 'top', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHreWYqvPgEf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAUVEGARDE DU MODÈLE FINE-TUNÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" SAUVEGARDE DU MODÈLE FINE-TUNÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sauvegarder le modèle et le tokenizer\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\" Modèle sauvegardé dans : {output_dir}\")\n",
        "\n",
        "# Sauvegarder les métriques du modèle fine-tuné\n",
        "finetuned_metrics = {\n",
        "    'model_name': model_name,\n",
        "    'dataset': 'allocine',\n",
        "    'train_size': len(train_df),\n",
        "    'test_size': len(test_df),\n",
        "    'accuracy': eval_results['eval_accuracy'],\n",
        "    'f1': eval_results['eval_f1'],\n",
        "    'precision': eval_results['eval_precision'],\n",
        "    'recall': eval_results['eval_recall'],\n",
        "    'f1_negative': eval_results['eval_f1_negative'],\n",
        "    'f1_positive': eval_results['eval_f1_positive'],\n",
        "    'loss': eval_results['eval_loss'],\n",
        "    'training_loss': train_result.training_loss,\n",
        "    'training_time': train_result.metrics['train_runtime'],\n",
        "    'num_epochs': training_args.num_train_epochs,\n",
        "    'learning_rate': training_args.learning_rate,\n",
        "    'batch_size': training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
        "}\n",
        "\n",
        "with open('finetuned_metrics_allocine.json', 'w') as f:\n",
        "    json.dump(finetuned_metrics, f, indent=2)\n",
        "\n",
        "print(f\" Métriques sauvegardées dans : finetuned_metrics_allocine.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MU2Oi-LPmO8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEST RAPIDE DU MODÈLE FINE-TUNÉ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" TEST RAPIDE DU MODÈLE FINE-TUNÉ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tester quelques exemples d'avis de films\n",
        "test_examples = [\n",
        "    \"Ce film est un véritable chef-d'œuvre ! L'histoire est captivante, les acteurs sont formidables et la réalisation est impeccable.\",\n",
        "    \"Film complètement raté. Scénario prévisible, dialogues creux et mise en scène bâclée. Une perte de temps totale.\",\n",
        "    \"Un bon divertissement sans prétention. Ça se regarde facilement mais sans plus, rien d'exceptionnel.\",\n",
        "    \"Magnifique ! Des images à couper le souffle, une bande sonore extraordinaire. Du grand cinéma !\",\n",
        "    \"Très déçu. Après toute cette attente, le film ne tient pas ses promesses. Ennuyeux et mal ficelé.\"\n",
        "]\n",
        "\n",
        "print(\" Prédictions du modèle fine-tuné sur de nouveaux avis :\")\n",
        "\n",
        "for i, text in enumerate(test_examples, 1):\n",
        "    # Tokeniser\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Prédire\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    print(f\"\\n{i}. \\\"{text}\\\"\")\n",
        "    print(f\"   → Prédiction: {label_mapping[predicted_class]}\")\n",
        "    print(f\"   → Confiance: {confidence:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bI38xSiNNKV"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RÉCAPITULATIF FINAL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" FINE-TUNING TERMINÉ AVEC SUCCÈS !\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\" Récapitulatif du projet :\")\n",
        "print(f\"   • Dataset: Allociné (avis de films français)\")\n",
        "print(f\"   • Modèle: {model_name}\")\n",
        "print(f\"   • Entraînement: {len(train_df)} exemples\")\n",
        "print(f\"   • Test: {len(test_df)} exemples\")\n",
        "print(f\"   • Performance finale: {eval_results['eval_accuracy']*100:.1f}% accuracy\")\n",
        "\n",
        "if baseline_metrics:\n",
        "    print(f\"   • Amélioration vs baseline: +{acc_improvement:.1f} points\")\n",
        "\n",
        "print(f\"\\n Fichiers générés :\")\n",
        "print(f\"   • {output_dir}/ : Modèle fine-tuné complet\")\n",
        "print(f\"   • finetuned_metrics_allocine.json : Métriques détaillées\")\n",
        "print(f\"   • logs/ : Logs d'entraînement TensorBoard\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}